# 爬虫介绍

**定义**

每时每刻，搜索引擎和网站都在采集大量信息，非原创即采集。采集信息用的程序一般被称为  网络爬虫（Web crawler）、网络铲（Web scraper，可类比考古用的洛阳铲）、网络蜘蛛（Web spider），其行为一般是先“爬”到对应的网页上，再把需要的信息“铲”下来。

**应用**

爬取数据用于数据挖掘与分析，数据建模，机器学习，网站测试，搜索引擎，推荐引擎...

**常用语言**

Python、Java、PHP、C#、Go...


**流程**

  * [发送请求](#发送请求) 
    * [get](#get)
    * [post](#post)
    * [session&cookie](#sessioncookie)
    * [user-agent](#user-agent)
    * [proxy](#proxy)
  * [解析网页](#解析网页)
    * [选择器](#选择器)
    * [正则表达式](#正则表达式)
  * [数据存储](#数据存储)
    * [SQL](#sql)
    * [CSV](#csv)
    * [json](#json)
  * [动态爬虫](#动态爬虫)
  * [爬虫框架](#爬虫框架)
  * 网页去重，反爬，api，增量爬取，多进程，异步，分布式爬取
  * [盗亦有道](#盗亦有道)


  
# 发送请求

  * ## get
    发送一个请求获得网页 
    ```python
    from urllib.request import urlopen
    response = urlopen("http://www.baidu.com")
    ```
  * ## post
    表单提交，通常用于登录网站
    ```python
    import requests
   
    data = {
    'form_email': 'xxxxxx@qq.com',
    'form_password': 'xxxx',
    }
    response = requests.post("https://accounts.douban.com/login", data=data)
    ```
  * ## session&cookie
    用以保存访问状态，如登录状态，回复状态
    ```python
    import requests
    session = requests.session()
    response = session.get("http://www.baidu.com")
    response = session.post("https://accounts.douban.com/login", data=data)
    ```
  * ## user-agent
    模拟浏览器请求头
    ```python
    headers = {
    'User-Agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"
    }
    response = requests.get("http://www.baidu.com", headers=headers)
    ```
  * ## proxy
    代理, 更换ip
    ```python
    proxies = {
    "http": "http://0.0.0.0:0000",
    "https": "http://1.1.1.1:1111",
    }
    response = requests.get("http://www.baidu.com", proxies=proxies)
    ```
# 解析网页
  * ## 选择器
    解析网页，抽取数据，从网页中获得其他链接 
    * beautifulsoup
    ```python
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html)
    soup.get_text()           #获得文本
    soup.h1                   #获得标题
    soup.span.parent          #获得span的父节点
    soup.span.next_sibling    #获得span的下一个兄弟节点
    soup.find_all("div", {"class": "item"}) #所有div里属性为item的标签
    ```
    * css selector(语法同css)
    ```python
    selector.css("#i")      #获得id名为i的标签
    selector.css(".i")      #获得属性名为i的标签
    selector.css("a::attr(href)")  #获得a标签里的链接
    ```
    * xpath selector
    ```python
    selector.xpath("div[@name='abc']") #获得div中属性name为abc的标签
    selector.xpath("//span")          #获得所有span标签
    ```
    * pyquery seletor(语法同jquery)
  * ## 正则表达式
    使用单个字符串来描述、匹配一系列符合某个句法规则的字符串
    ```python
    re.match(r"^[0-9a-zA-Z_]{0,19}@[0-9a-zA-Z]{1,13}\.[com,cn,net]{1,3}$", str)   #匹配所有邮箱
    re.match(r"^1(3[0-9]|4[57]|5[0-35-9]|7[0135678]|8[0-9])\d{8}$", str)   #匹配所有手机号 
    ```
# 数据存储
  * ## SQL
    ```python
    import MySQLdb
    connect = MySQLdb.connect('0.0.0.0', 'root', 'root', 'douban', charset='utf8', use_unicode=True)
    cursor = connect.cursor()
    insert_sql = '''insert into douban_top250(Chinese_title,English_title,star_num) 
                    VALUES ("%s","%s","%s") ;
                 '''%(Chinese_title,English_title,star_num)
    cursor.execute(insert_sql)
    connect.commit()
    connect.close()
    ```
  * ## json
    ```python
    import json
    json_file=open('douban.json','w',encoding='utf-8')
    dic = {
            'Chinese_title': Chinese_title,
            'English_title': English_title,
            'star_num':star_num
        }
    json.dump(dic, json_file, ensure_ascii=False, separators=(',', ': '))
    json_file.write(",\n")
    json_file.close()
    ```
  * ## CSV
    ```python
    import csv
    csv_file = open('douban.csv', 'w',encoding='utf-8')
    spamwriter = csv.writer(csv_file, delimiter=' ', quoting=csv.QUOTE_MINIMAL)
    row = [Chinese_title, English_title, star_num]
    spamwriter.writerow(row)
    csv_file.close()
    ```
# 动态爬虫
爬取有*js*的网页
  * selenium
    ```python
    from selenium import webdriver
    browser = webdriver.Chrome("F://chromedriver.exe")
    browser.get("https://www.douban.com/accounts/login")
    browser.find_element_by_css_selector("#email").send_keys("xxxxxxx@qq.com")
    browser.find_element_by_css_selector("#password").send_keys("xxxxxx")
    time.sleep(1)
    browser.find_element_by_css_selector(".btn-submit").click()
    ```
  * Splash
  * PyV8
  * Ghost
  * execjs
# 爬虫框架
  * Scrapy
  * Crawley
  * Portia
  * Newspaper
# 盗亦有道
  * robots 协议

